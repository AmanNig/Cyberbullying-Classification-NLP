{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPqEGaAlS05NxjS1KlZIv7y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmanNig/Cyberbullying-Classification-NLP/blob/main/220118_AMAN_MODEL_END_TERM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('omw-1.4')\n",
        "import spacy\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import word_tokenize,sent_tokenize # tokenizing\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "#stop-words\n",
        "stop_words=set(nltk.corpus.stopwords.words('english'))\n",
        "# Graphs\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import plotly.figure_factory as ff\n",
        "from plotly.subplots import make_subplots\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# vectorizers for creating the document-term-matrix (DTM)\n",
        "from sklearn.model_selection import train_test_split,cross_validate\n",
        "from sklearn.metrics import accuracy_score,roc_auc_score,f1_score,classification_report,precision_score, recall_score\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "# ML Models\n",
        "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
        "from xgboost import XGBClassifier\n",
        "#keras #tf\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.preprocessing.text import one_hot,Tokenizer, text_to_word_sequence\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense , Flatten ,Embedding,Input,CuDNNLSTM,LSTM, BatchNormalization, Dropout, InputLayer, ReLU\n",
        "from keras.initializers import Constant\n",
        "from keras.callbacks import EarlyStopping\n",
        "import keras_tuner as kt\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "F1_Score_tf = tfa.metrics.F1Score(num_classes=1,average=\"weighted\")\n",
        "auc_roc_metric_ROC = keras.metrics.AUC(curve='ROC')\n",
        "from keras import backend as K\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def F1_Score_tf(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "    auc_roc_metric_ROC = keras.metrics.AUC(curve='ROC')\n",
        "\n",
        "\n",
        "#gensim w2v  #word2vec\n",
        "import gensim\n",
        "from gensim.models import Word2Vec,KeyedVectors\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize"
      ],
      "metadata": {
        "id": "8qonRopZOcfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"/kaggle/input/cyberbullying-dataset/twitter_parsed_dataset.csv\")\n",
        "data.dropna(inplace=True)\n",
        "data.reset_index(inplace=True,drop=True)\n",
        "data.drop(columns=['index','id','Annotation'],inplace=True)\n",
        "data"
      ],
      "metadata": {
        "id": "csh8tgpJOuDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I8ugJjCxUWqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_chars(text):\n",
        "    return len(text)\n",
        "\n",
        "# count number of words\n",
        "def count_words(text):\n",
        "    return len(text.split())\n",
        "\n",
        "# count number of capital characters\n",
        "def count_capital_chars(text):\n",
        "    count=0\n",
        "    for i in text:\n",
        "        if i.isupper():\n",
        "            count+=1\n",
        "    return count\n",
        "\n",
        "# count number of capital words\n",
        "def count_capital_words(text):\n",
        "  def count_capital_words(text):\n",
        "    return sum(map(str.isupper,text.split()))\n",
        "\n",
        "# count number of punctuations\n",
        "def count_punctuations(text):\n",
        "    punctuations='!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
        "    d=0\n",
        "    for i in punctuations:\n",
        "        d = d + text.count(i)\n",
        "    return d\n",
        "\n",
        "# count number of words in quotes\n",
        "def count_words_in_quotes(text):\n",
        "    x = re.findall(\"\\'.\\'|\\\".\\\"\", text)\n",
        "    count=0\n",
        "    if x is None:\n",
        "        return 0\n",
        "    else:\n",
        "        for i in x:\n",
        "            t=i[1:-1]\n",
        "            count+=count_words(t)\n",
        "        return count\n",
        "\n",
        "# count number of sentences\n",
        "def count_sent(text):\n",
        "    return len(nltk.sent_tokenize(text))\n",
        "\n",
        "# calculate average sentence length\n",
        "def avg_sent_len(word_cnt,sent_cnt):\n",
        "    return word_cnt/sent_cnt\n",
        "\n",
        "# count number of unique words\n",
        "def count_unique_words(text):\n",
        "    return len(set(text.split()))\n",
        "\n",
        "# words vs unique feature\n",
        "def words_vs_unique(words,unique):\n",
        "    return unique/words\n",
        "    def count_htags(text):\n",
        "    x = re.findall(r'(\\#\\w[A-Za-z0-9]*)', text)\n",
        "    return len(x)\n",
        "\n",
        "# count of mentions\n",
        "def count_mentions(text):\n",
        "    x = re.findall(r'(\\@\\w[A-Za-z0-9]*)', text)\n",
        "    return len(x)\n",
        "\n",
        "# count of stopwords\n",
        "def count_stopwords(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    word_tokens = word_tokenize(text)\n",
        "    stopwords_x = [w for w in word_tokens if w in stop_words]\n",
        "    return len(stopwords_x)\n",
        "    def stopwords_vs_words(stopwords_cnt,text):\n",
        "     return stopwords_cnt/len(word_tokenize(text))"
      ],
      "metadata": {
        "id": "f4zDFH7pOzKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['char_count'] = data[\"Text\"].apply(lambda x:count_chars(str(x)))\n",
        "data['word_count'] = data[\"Text\"].apply(lambda x:count_words(str(x)))\n",
        "data['sent_count'] = data[\"Text\"].apply(lambda x:count_sent(str(x)))\n",
        "data['capital_char_count'] = data[\"Text\"].apply(lambda x:count_capital_chars(str(x)))\n",
        "data['capital_word_count'] = data[\"Text\"].apply(lambda x:count_capital_words(str(x)))\n",
        "data['quoted_word_count'] = data[\"Text\"].apply(lambda x:count_words_in_quotes(str(x)))\n",
        "data['stopword_count'] = data[\"Text\"].apply(lambda x:count_stopwords(str(x)))\n",
        "data['unique_word_count'] = data[\"Text\"].apply(lambda x:count_unique_words(str(x)))\n",
        "data['htag_count'] = data[\"Text\"].apply(lambda x:count_htags(str(x)))\n",
        "data['mention_count'] = data[\"Text\"].apply(lambda x:count_mentions(str(x)))\n",
        "data['punct_count'] = data[\"Text\"].apply(lambda x:count_punctuations(str(x)))\n",
        "data['avg_wordlength']=data['char_count']/data['word_count']\n",
        "data['avg_sentlength']=data['word_count']/data['sent_count']\n",
        "data['unique_vs_words']=data['unique_word_count']/data['word_count']\n",
        "data['stopwords_vs_words']=data['stopword_count']/data['word_count']"
      ],
      "metadata": {
        "id": "Ic6y47_wPM2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[\"Text\"][145:155].values\n",
        "data[\"Text\"][2550:2560].values"
      ],
      "metadata": {
        "id": "Ti2AA5dZPFzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('/kaggle/input/emoji-dictionary-1/Emoji_Dict.p', 'rb') as fp:\n",
        "    Emoji_Dict = pickle.load(fp)\n",
        "Emoji_Dict = {v: k for k, v in Emoji_Dict.items()}\n",
        "\n",
        "with open('/kaggle/input/emotions-dictionary-for-nlp/Emoticon_Dict.p', 'rb') as fp:\n",
        "    Emoticon_Dict = pickle.load(fp)\n",
        "Emoticon_Dict = {v: k for k, v in Emoticon_Dict.items()}\n",
        "\n",
        "def convert_emojis_to_word(text):\n",
        "    for emot in Emoji_Dict:\n",
        "        text = re.sub(r'('+emot+')', \"_\".join(Emoji_Dict[emot].replace(\",\",\"\").replace(\":\",\"\").split()), text)\n",
        "    return text\n",
        "\n",
        "def convert_emoticons(text):\n",
        "    for emot in Emoticon_Dict:\n",
        "        text = re.sub(u'('+emot+')', \"_\".join(Emoticon_Dict[emot].replace(\",\",\"\").split()), text)\n",
        "    return text\n",
        "    import pickle\n",
        "with open('/kaggle/input/emoji-dictionary-1/Emoji_Dict.p', 'rb') as fp:\n",
        "    Emoji_Dict = pickle.load(fp)\n",
        "Emoji_Dict = {v: k for k, v in Emoji_Dict.items()}\n",
        "\n",
        "with open('/kaggle/input/emotions-dictionary-for-nlp/Emoticon_Dict.p', 'rb') as fp:\n",
        "    Emoticon_Dict = pickle.load(fp)\n",
        "Emoticon_Dict = {v: k for k, v in Emoticon_Dict.items()}\n",
        "\n",
        "def convert_emojis_to_word(text):\n",
        "    for emot in Emoji_Dict:\n",
        "        text = re.sub(r'('+emot+')', \"_\".join(Emoji_Dict[emot].replace(\",\",\"\").replace(\":\",\"\").split()), text)\n",
        "    return text\n",
        "\n",
        "def convert_emoticons(text):\n",
        "    for emot in Emoticon_Dict:\n",
        "        text = re.sub(u'('+emot+')', \"_\".join(Emoticon_Dict[emot].replace(\",\",\"\").split()), text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "BeF6sPunPccQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "tqdm.pandas(desc=\"my bar!\")\n",
        "\n",
        "column = \"Text\"\n",
        "data[column] = data[column].progress_apply(lambda text: convert_emoticons(str(text)))\n",
        "data[column] = data[column].progress_apply(lambda text: convert_emojis_to_word(str(text)))\n",
        "print( data[\"Text\"][145:155].values )\n",
        "print( data[\"Text\"][2550:2560].values )"
      ],
      "metadata": {
        "id": "VgMCPQ4yPkjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_copy = data.copy()\n",
        "data = data_copy.copy()\n",
        "def remove_urls(text):\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url_pattern.sub(r'', text)\n",
        "\n",
        "def remove_html(text):\n",
        "    html_pattern = re.compile('<.*?>')\n",
        "    return html_pattern.sub(r'', text)\n",
        "\n",
        "data[column] = data[column].apply(lambda text: remove_urls(str(text)))\n",
        "data[column] = data[column].apply(lambda text: remove_html(str(text)))\n",
        "data"
      ],
      "metadata": {
        "id": "8z_vx-lQPzDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_words_map_dict = {}\n",
        "chat_words_list = []\n",
        "for line in chat_words_str.split(\"\\n\"):\n",
        "    if line != \"\":\n",
        "        cw = line.split(\"=\")[0]\n",
        "        cw_expanded = line.split(\"=\")[1]\n",
        "        chat_words_list.append(cw)\n",
        "        chat_words_map_dict[cw] = cw_expanded\n",
        "chat_words_list = set(chat_words_list)\n",
        "\n",
        "def chat_words_conversion(text):\n",
        "    new_text = []\n",
        "    for w in text.split():\n",
        "        if w.upper() in chat_words_list:\n",
        "            new_text.append(chat_words_map_dict[w.upper()])\n",
        "        else:\n",
        "            new_text.append(w)\n",
        "    return \" \".join(new_text)\n",
        "\n",
        "data[column] = data[column].progress_apply(lambda text: chat_words_conversion(str(text)))\n",
        "data"
      ],
      "metadata": {
        "id": "cQDv_G0dQDRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_numbers(text):\n",
        "    number_pattern = r'\\d+'\n",
        "    without_number = re.sub(pattern=number_pattern, repl=\" \", string=text)\n",
        "    return without_number\n",
        "\n",
        "data[column] = data[column].progress_apply(lambda text: remove_numbers(str(text)))\n",
        "def make_lowercase(data,column):\n",
        "    data[column] = data[column].str.lower()\n",
        "    return data\n",
        "data = make_lowercase(data,column='Text')"
      ],
      "metadata": {
        "id": "ghrGSDEzQKFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "data[\"sentiment\"] = data[\"Text\"].progress_apply(lambda x: TextBlob(x).sentiment.polarity)\n",
        "data"
      ],
      "metadata": {
        "id": "m6IRljaCQbZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "no_words = set(['no',\"wasn't\",\"has't\",\"didn't\",\"hadn't\",\"mightn't\",\"needn't\",\"won't\",\"weren't\",\"doesn't\",\"mustn't\",\"shouldn't\",\"haven't\",\"doesn\",\"hadn\",\"didn't\",\"not\",\"needn\",\"aren't\",\"isn\",\"aren\",\"isn't\",\"wasn\",\"hasn't\",\"didn\",\"shouldn\",\"don't\",\"couldn't\",\"wouldn't\",\"weren\",\"hasn\",\"\"])\n",
        "stop_words_n = set(stopwords.words('english'))-no_words\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    return \" \".join([word for word in str(text).split() if word not in stop_words_n])\n",
        "\n",
        "data[\"Text\"] = data[\"Text\"].progress_apply(lambda text: remove_stopwords(text))\n",
        "def remove_AtSign_tags(data,column):\n",
        "    data[column] = data[column].apply( lambda text: re.sub('\\@[a-zA-Z]+','',text) )\n",
        "    return data\n",
        "\n",
        "data = remove_AtSign_tags(data,column='Text')\n",
        "def remove_puntuations(data,column):\n",
        "    data[column] = data[column].progress_apply(lambda text: str(text).translate(str.maketrans('','',string.punctuation)))\n",
        "    return data\n",
        "data = remove_puntuations(data,\"Text\")\n",
        "def remove_single_char(text):\n",
        "    single_char_pattern = r'\\s+[a-zA-Z]\\s+'\n",
        "    without_sc = re.sub(pattern=single_char_pattern, repl=\" \", string=text)\n",
        "    return without_sc\n",
        "\n",
        "data[column] = data[column].progress_apply(lambda text: remove_single_char(str(text)))"
      ],
      "metadata": {
        "id": "mAYtgsUxQeWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_most_frequent_words(data,column,frequent_words_count):\n",
        "    all_text = ' '.join(data[column].tolist())\n",
        "    cnt = Counter(all_text.split())\n",
        "    most_frequent_words = cnt.most_common(frequent_words_count)\n",
        "    # print(most_frequent_words)\n",
        "    data[column] = data[column].progress_apply(lambda text: \" \".join(list(set(text.split()) - set(most_frequent_words))))\n",
        "    return data\n",
        "\n",
        "def remove_rare_words(data,column,rare_words_count):\n",
        "    all_text = ' '.join(data[column].tolist())\n",
        "    cnt = Counter(all_text.split())\n",
        "    most_frequent_words = cnt.most_common()\n",
        "    rare_words = most_frequent_words[len(most_frequent_words)-rare_words_count:]\n",
        "    # print(rare_words)\n",
        "    rare_words = set(dict(rare_words).keys())\n",
        " data[column] = data[column].progress_apply(lambda text: \" \".join(list(set(text.split()) - rare_words)))\n",
        "    return data\n",
        "\n",
        "data = remove_most_frequent_words(data,column='Text',frequent_words_count=20)\n",
        "data = remove_rare_words(data,column=\"Text\",rare_words_count=1000)"
      ],
      "metadata": {
        "id": "V3cWzhuDQ3Nf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "column = \"Text\"\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n",
        "def lemmatize_words(text):\n",
        "    pos_tagged_text = nltk.pos_tag(text.split())\n",
        "    return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n",
        "data[column] = data[column].progress_apply(lambda text: lemmatize_words(text))\n",
        "data"
      ],
      "metadata": {
        "id": "kAGVBrUVQ--l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['oh_label'] = data['oh_label'].astype(int)\n",
        "\n",
        "index = []\n",
        "for i in tqdm(list(data.index)):\n",
        "    count = len(data['Text'][i].split())\n",
        "    if (count<=3) & (data['oh_label'][i]==0):\n",
        "        index.append(i)\n",
        "\n",
        "data.drop(index=index,inplace=True)"
      ],
      "metadata": {
        "id": "v24VldU-RFWC",
        "outputId": "6687a171-bac5-4808-eda9-1018da0e2fa2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f7bce6822836>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'oh_label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'oh_label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.to_csv(\"preprocessed_text_twitter_parsed_dataset.csv\")\n",
        "data = pd.read_csv(\"/kaggle/working/preprocessed_text_twitter_parsed_dataset.csv\")\n",
        "data.drop(columns=\"Unnamed: 0\",inplace=True)\n",
        "data.dropna(axis=\"index\",inplace=True)\n",
        "data"
      ],
      "metadata": {
        "id": "4R7sSi25RIri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Plot_Loss_Accuracy( history, epochs ):\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    auc = history.history['accuracy']\n",
        "    val_auc = history.history['val_accuracy']\n",
        "\n",
        "    epochs = np.linspace(1,epochs,epochs)\n",
        "    epochs\n",
        "\n",
        "    fig = go.Figure(data=go.Scatter( x=epochs, y=loss, name='Loss' ))\n",
        "    fig.add_trace( go.Scatter( x=epochs, y=val_loss, name='Validation Lass' ) )\n",
        "    fig.layout.update( xaxis_title = 'No. of Epochs',\n",
        "                       yaxis_title = \"Loss\",\n",
        "                       legend=dict( yanchor=\"bottom\", y=0.01, xanchor=\"right\", x=0.99),\n",
        "                       width=750, height=650,\n",
        "                       font=dict(size=18))\n",
        "    fig.show()\n",
        "\n",
        "    fig = go.Figure(data=go.Scatter( x=epochs, y=auc, name='Accuracy Score' ))\n",
        "    fig.add_trace( go.Scatter( x=epochs, y=val_auc, name='Validation Accuracy Score' ) )\n",
        "    fig.layout.update( xaxis_title = 'No. of Epochs',\n",
        "                       yaxis_title = \"Accuracy Scores\",\n",
        "                       legend=dict( yanchor=\"bottom\", y=0.01, xanchor=\"right\", x=0.99),\n",
        "                       width=750, height=650,\n",
        "                       font=dict(size=18))\n",
        "    fig.show()"
      ],
      "metadata": {
        "id": "eolrbOuESDz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Evaluation(model,X_train,X_test,y_train,y_test,hypertuning=False):\n",
        "\n",
        "  plt.figure(  figsize=(12,6) )\n",
        " #print( \"-----------------------------------------------------------------------------------------------------------\")\n",
        " # print( model )\n",
        " # print( \" For Train Set :  \")\n",
        "  y_pred = model.predict(X_train)\n",
        "  y_pred_proba = model.predict_proba(X_train)\n",
        "\n",
        "  accuracy_train = accuracy_score( y_train, y_pred )\n",
        "  precision_train = precision_score( y_train, y_pred )\n",
        "  recall_train = recall_score(y_train, y_pred)\n",
        "  F1_score_train = f1_score(y_train, y_pred)\n",
        "  #print(\"F1_Score = \", F1_score_train )\n",
        "  roc_auc_train = roc_auc_score(y_train, y_pred_proba[:,1])\n",
        "  #print( classification_report( y_train, y_pred ) )\n",
        "\n",
        "  #print( \" For Test Set :  \")\n",
        "  y_pred = model.predict(X_test)\n",
        "  y_pred_proba = model."
      ],
      "metadata": {
        "id": "XxP1KBIBUYvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_models_with_default_paramters(X_train,X_test,y_train,y_test):\n",
        "  models_default = [ XGBClassifier(),\n",
        "                     GaussianNB(), #MultinomialNB()\n",
        "                     RandomForestClassifier(),\n",
        "                     DecisionTreeClassifier(),\n",
        "                     # ExtraTreeClassifier(), KNeighborsClassifier(),\n",
        "                     # SVC(probability=True),\n",
        "                     AdaBoostClassifier(),\n",
        "                     ]\n",
        "\n",
        "\n",
        "  F1_Score_train = []\n",
        "  Accuracy_train = []\n",
        "  Recall_train = []\n",
        "  Precision_train = []\n",
        "  ROC_AUC_Score_train = []\n",
        "\n",
        "\n",
        "  F1_Score_test = []\n",
        "  Accuracy_test = []\n",
        "  Recall_test = []\n",
        "  Precision_test = []\n",
        "  ROC_AUC_Score_test = []\n",
        "\n",
        "  Model_Name = []\n",
        "\n",
        "  for model in models_default:\n",
        "    # print(model)\n",
        "    Model_Name.append( model )\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    accuracy_train, precision_train, recall_train, F1_score_train, roc_auc_train, accuracy_test, precision_test, recall_test, F1_score_test, roc_auc_test = Evaluation(model,X_train,X_test,y_train,y_test,False)\n",
        "\n",
        "    F1_Score_train.append( F1_score_train )\n",
        "    Accuracy_train.append( accuracy_train )\n",
        "    Recall_train.append( recall_train )\n",
        "    Precision_train.append( precision_train )\n",
        "    ROC_AUC_Score_train.append( roc_auc_train )\n",
        "\n",
        "    F1_Score_test.append( F1_score_test )\n",
        "    Accuracy_test.append( accuracy_test)\n",
        "      Recall_test.append( recall_test )\n",
        "    Precision_test.append( precision_test )\n",
        "    ROC_AUC_Score_test.append( roc_auc_test )\n",
        "\n",
        "  results = pd.DataFrame()\n",
        "  results['Model_Name'] = Model_Name\n",
        "\n",
        "  train_test_f1_score_difference = np.subtract(F1_Score_train,F1_Score_test)  # To Check Overfitting/Underfitting\n",
        "\n",
        "  results['F1_Score on Test Set'] = F1_Score_test\n",
        "  results['Accuracy on Test Set'] = Accuracy_test\n",
        "  results['Recall on Test Set'] = Recall_test\n",
        "  results['Precision on Test Set'] = Precision_test\n",
        "  results['ROC_AUC_Score on Test Set'] = ROC_AUC_Score_test\n",
        "\n",
        "  results['F1_Score on Train Set'] = F1_Score_train\n",
        "  results['Accuracy on Train Set'] = Accuracy_train\n",
        "  results['Recall on Train Set'] = Recall_train\n",
        "  results['Precision on Train Set'] = Precision_train\n",
        "  results['ROC_AUC_Score on Train Set'] = ROC_AUC_Score_train\n",
        "\n",
        "  results['Difference of F1_Score on train and test'] = train_test_f1_score_difference\n",
        " results = results.sort_values(by=['F1_Score on Test Set','Difference of F1_Score on train and test'],ascending = [False, True])\n",
        "\n",
        "  return results\n"
      ],
      "metadata": {
        "id": "nMA4LoPtUYy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.pie(values=[sum(data['oh_label']==0),sum(data['oh_label']==1)], names=['no(0)','yes(1)'] , title='Target')\n",
        "fig.update_layout(width=300,height=300)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "TDHP5DP1VKbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"/kaggle/working/preprocessed_text_twitter_parsed_dataset.csv\")\n",
        "data.drop(columns=\"Unnamed: 0\",inplace=True)\n",
        "data.dropna(axis=\"index\",inplace=True)\n",
        "data\n",
        "\n",
        "# # # Taking sample temporary sample of data for writting code\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# data_bigger, data  = train_test_split(data, test_size=0.001, random_state=10, stratify=data['oh_label'])\n",
        "# data.reset_index(inplace=True)\n",
        "# data.drop(columns=['index'],inplace=True)\n",
        "# data\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.drop(columns=[\"oh_label\"]), data[['oh_label']], test_size=0.2, random_state=10, stratify=data['oh_label'])\n",
        "X_train.reset_index( inplace=True, drop=True ), X_test.reset_index( inplace=True, drop=True ), y_train.reset_index( inplace=True, drop=True ), y_test.reset_index( inplace=True, drop=True )"
      ],
      "metadata": {
        "id": "Tp5tPRVOVQg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer( lowercase=True, ngram_range=(1, 1), max_features=300,\n",
        "                               binary=False, stop_words = 'english' )\n",
        "\n",
        "X_train_text = pd.DataFrame.sparse.from_spmatrix( vectorizer.fit_transform(X_train['Text']) )\n",
        "X_test_text = pd.DataFrame.sparse.from_spmatrix( vectorizer.transform(X_test['Text']) )\n",
        "# vectorizer.get_feature_names_out()\n",
        "\n",
        "X_train = pd.concat([X_train_text,X_train.drop(columns=['Text'])], axis=1)\n",
        "X_test = pd.concat([X_test_text,X_test.drop(columns=['Text'])],axis=1)\n",
        "Results = apply_models_with_default_paramters(X_train,X_test,y_train,y_test)\n",
        "Results"
      ],
      "metadata": {
        "id": "9zokedr3VYvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(512, input_dim=X_train.shape[1], activation=tf.keras.layers.ReLU()))\n",
        "model.add(Dense(256, activation=tf.keras.layers.ReLU()))\n",
        "model.add(Dense(64, activation=tf.keras.layers.ReLU()))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy' , optimizer='adam', metrics=['accuracy', F1_Score_tf])\n",
        "history = model.fit(X_train, y_train , validation_data=(X_test,y_test), epochs=20, batch_size=32)\n",
        "\n",
        "Plot_Loss_Accuracy( history, epochs=20 )"
      ],
      "metadata": {
        "id": "yS98yRNLVhaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"/kaggle/working/preprocessed_text_twitter_parsed_dataset.csv\")\n",
        "data.drop(columns=\"Unnamed: 0\",inplace=True)\n",
        "data.dropna(axis=\"index\",inplace=True)\n",
        "data\n"
      ],
      "metadata": {
        "id": "-kLzhheRVspG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_pre_model = KeyedVectors.load_word2vec_format('/kaggle/input/googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin',binary=True,limit=100000)\n",
        "vocab = w2v_pre_model.key_to_index\n",
        "# vec = w2v_pre_model['man']\n",
        "# print(vec)\n",
        "w2v_pre_model.most_similar('man')"
      ],
      "metadata": {
        "id": "p3857TziVz5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(data['oh_label']==0).sum(), (data['oh_label']==1).sum()"
      ],
      "metadata": {
        "id": "gd31bE7iV6g6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def document_vector(text,model):\n",
        "    # remove out-of-vocabulary words\n",
        "    doc = [word for word in text if word in vocab]\n",
        "    if len(doc)==0:\n",
        "        mean = np.nan\n",
        "    else:\n",
        "        mean = np.mean(model[doc], axis=0)\n",
        "    return mean\n",
        "\n",
        "\n",
        "# document_vector(data['Text'][0],w2v_pre_model)\n",
        "data['Text_Vec'] = data['Text'].apply( lambda text: document_vector(text.split(),w2v_pre_model))\n",
        "data.dropna(inplace=True)\n",
        "data.reset_index(inplace=True)\n",
        "data.drop(columns=[\"index\"],inplace=True)\n",
        "data.head()"
      ],
      "metadata": {
        "id": "MLLntvZwV-qk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(data['oh_label']==0).sum(), (data['oh_label']==1).sum()"
      ],
      "metadata": {
        "id": "F-RQgP1uWJWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(np.zeros(shape=(data.shape[0],300)),columns=[\"col_\"+str(i) for i in range(300)])\n",
        "\n",
        "for i in range(data.shape[0]):\n",
        "    try:\n",
        "        df.iloc[i]=data['Text_Vec'][i]# .append(data['oh_label'][i])\n",
        "    except:\n",
        "        df.iloc[i] = [np.nan for i in range(300)]\n",
        "\n",
        "df['target'] = data[\"oh_label\"]\n",
        "df.head()"
      ],
      "metadata": {
        "id": "mAmptuzzWQua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,X_test,y_train,y_test = train_test_split(df.drop(columns=['target']),df[['target']],test_size=0.2,random_state=10,stratify=df[['target']])\n",
        "X_train.shape,X_test.shape,y_train.shape,y_test.shape\n",
        "Results = apply_models_with_default_paramters(X_train,X_test,y_train,y_test)\n",
        "Results"
      ],
      "metadata": {
        "id": "jgpoV9NPWYV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(128, input_dim=X_train.shape[1], activation=tf.keras.layers.LeakyReLU(alpha=0.05)))\n",
        "model.add(Dense(64, input_dim=X_train.shape[1], activation=tf.keras.layers.LeakyReLU(alpha=0.05)))\n",
        "model.add(Dense(32, input_dim=X_train.shape[1], activation=tf.keras.layers.LeakyReLU(alpha=0.05)))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy' , optimizer='adam', metrics=['accuracy', auc_roc_metric_ROC, F1_Score_tf])\n",
        "history = model.fit(X_train, y_train , validation_data=(X_test,y_test), epochs=10, batch_size=32)\n",
        "\n",
        "Plot_Loss_Accuracy( history, epochs=10 )"
      ],
      "metadata": {
        "id": "hMpumuZSWiLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"/kaggle/working/preprocessed_text_twitter_parsed_dataset.csv\")\n",
        "data.drop(columns=\"Unnamed: 0\",inplace=True)\n",
        "data.dropna(axis=\"index\",inplace=True)\n",
        "data"
      ],
      "metadata": {
        "id": "G7-HG38gWpLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 300\n",
        "sentences_list = data['Text'].to_list()\n",
        "# sentences_list[0:3]\n",
        "w2v_model = gensim.models.Word2Vec(sentences=sentences_list,window=5,min_count=1,vector_size=embed_dim)\n",
        "w2v_model.train(sentences_list,epochs=25,total_examples=len(sentences_list))"
      ],
      "metadata": {
        "id": "btkKp-v4WuaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_len = w2v_model.wv.vectors.shape[0]\n",
        "print(\"The total number of words are : \", vocab_len )\n",
        "vocab = w2v_model.wv.key_to_index.keys()"
      ],
      "metadata": {
        "id": "v27eysfBWvn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def document_vector(text,model):\n",
        "    # remove out-of-vocabulary words\n",
        "    doc = [word for word in text if word in vocab]\n",
        "    if len(doc)==0:\n",
        "        mean = np.nan\n",
        "    else:\n",
        "        mean = np.mean(model.wv[doc], axis=0)\n",
        "    return mean\n",
        "    data['Text_Vec'] = data['Text'].apply( lambda text: document_vector(text.split(),w2v_model))\n",
        "data.dropna(inplace=True)\n",
        "data.reset_index(inplace=True)\n",
        "data.drop(columns=[\"index\"],inplace=True)"
      ],
      "metadata": {
        "id": "uhI2DsYKW-6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(np.zeros(shape=(data.shape[0],300)),columns=[\"col_\"+str(i) for i in range(300)])\n",
        "\n",
        "for i in range(data.shape[0]):\n",
        "    try:\n",
        "        df.iloc[i]=data['Text_Vec'][i]# .append(data['oh_label'][i])\n",
        "    except:\n",
        "        df.iloc[i] = [np.nan for i in range(300)]\n",
        "\n",
        "df['target'] = data[\"oh_label\"]\n",
        "df.head()"
      ],
      "metadata": {
        "id": "SpejA5UpXCVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,X_test,y_train,y_test = train_test_split(df.drop(columns=['target']),df['target'],test_size=0.2,random_state=10,stratify=df['target'])\n",
        "X_train.shape,X_test.shape,y_train.shape,y_test.shape\n",
        "Results = apply_models_with_default_paramters(X_train,X_test,y_train,y_test)\n",
        "Results"
      ],
      "metadata": {
        "id": "7KpQgG27XMUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(128, input_dim=X_train.shape[1], activation=tf.keras.layers.LeakyReLU(alpha=0.05)))\n",
        "model.add(Dense(64, input_dim=X_train.shape[1], activation=tf.keras.layers.LeakyReLU(alpha=0.05)))\n",
        "model.add(Dense(32, input_dim=X_train.shape[1], activation=tf.keras.layers.LeakyReLU(alpha=0.05)))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy' , optimizer='adam', metrics=['accuracy', auc_roc_metric_ROC])\n",
        "history = model.fit(X_train, y_train , validation_data=(X_test,y_test), epochs=10, batch_size=32)\n",
        "\n",
        "Plot_Loss_Accuracy( history, epochs=10 )"
      ],
      "metadata": {
        "id": "SP_UEQRbXNJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"/kaggle/working/preprocessed_text_twitter_parsed_dataset.csv\")\n",
        "data.drop(columns=\"Unnamed: 0\",inplace=True)\n",
        "data.dropna(axis=\"index\",inplace=True)\n",
        "data"
      ],
      "metadata": {
        "id": "JYK7mpX1XaIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tok = Tokenizer()\n",
        "tok.fit_on_texts(data['Text'])\n",
        "vocab_size = len(tok.word_index) + 1\n",
        "print(\"Vocabulary Size: \", vocab_size )"
      ],
      "metadata": {
        "id": "QHoDPDuhXbJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_sequence_and_padding(tokenizer, data, column):\n",
        "    sequences = tokenizer.texts_to_sequences(data[column]) # convert words into sequences\n",
        "    max_length = int(max([len(vector) for vector in sequences ])/4)\n",
        "    sequences = pad_sequences( sequences, maxlen=max_length, padding='post')\n",
        "    return sequences, max_length\n",
        "\n",
        "sequences, max_length = encode_sequence_and_padding(tok, data=data, column='Text')\n",
        "sequences = pd.DataFrame(sequences)\n",
        "print(\"Maximum length of Sentence: \",max_length)\n",
        "sequences.to_csv(\"sequences.csv\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(sequences, data['oh_label'], test_size=0.20, random_state = 10, stratify=data['oh_label'])"
      ],
      "metadata": {
        "id": "RSlQWlmjXfYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding( vocab_size, 300, input_length = max_length, mask_zero=True ))\n",
        "model.add(LSTM(256, return_sequences=True))\n",
        "model.add(LSTM(128, return_sequences=True))\n",
        "model.add(LSTM(64))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy',auc_roc_metric_ROC, F1_Score_tf])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "06jblAWqXshH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs=5\n",
        "batch_size=32\n",
        "history = model.fit(X_train,y_train,epochs=epochs,batch_size=batch_size,validation_data=(X_test,y_test))\n",
        "Plot_Loss_Accuracy( history, epochs=epochs )"
      ],
      "metadata": {
        "id": "2FOJa103Xtrs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}